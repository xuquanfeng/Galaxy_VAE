{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dde6e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.10.0+cu113\n",
      "Torchvision Version:  0.11.1+cu113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xqf/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Nov 22 20:18:19 2021\n",
    "\n",
    "@author: xuquanfeng\n",
    "\"\"\"\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import datasets,transforms,utils,models\n",
    "from VAE_model.models import VAE, MyDataset\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import datetime\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from astropy.io import fits\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "#设置随机种子\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "setup_seed(10)\n",
    "# Hyper parameters\n",
    "if not os.path.exists('./model'):\n",
    "    os.mkdir('./model')\n",
    "if not os.path.exists('./train_proces'):\n",
    "    os.mkdir('./train_proces')\n",
    "num_epochs = 20   #循环次数\n",
    "batch_size = 128    #每次投喂数据量\n",
    "learning_rate = 0.00001   #学习率\n",
    "num_var = 40\n",
    "momentum = 0.8\n",
    "k = 1\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model = torch.load('/data/xqf/VAE3/model/vae_40_best.pth')\n",
    "\n",
    "# print(model)\n",
    "# Device configuration  判断能否使用cuda加速\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "reconstruction_function = nn.MSELoss(size_average=False)\n",
    "\n",
    "def loss_function1(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    BCE = reconstruction_function(recon_x, x)  # mse loss\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    # KL divergence\n",
    "    return BCE + k*KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebd079a0-e612-4556-9472-307fcfc98b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMDLoss(nn.Module):\n",
    "    '''\n",
    "    计算源域数据和目标域数据的MMD距离\n",
    "    Params:\n",
    "    source: 源域数据（n * len(x))\n",
    "    target: 目标域数据（m * len(y))\n",
    "    kernel_mul:\n",
    "    kernel_num: 取不同高斯核的数量\n",
    "    fix_sigma: 不同高斯核的sigma值\n",
    "    Return:\n",
    "    loss: MMD loss\n",
    "    '''\n",
    "    def __init__(self, kernel_type='rbf', kernel_mul=2.0, kernel_num=5, fix_sigma=None, **kwargs):\n",
    "        super(MMDLoss, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.fix_sigma = None\n",
    "        self.kernel_type = kernel_type\n",
    "\n",
    "    def guassian_kernel(self, source, target, kernel_mul, kernel_num, fix_sigma):\n",
    "        n_samples = int(source.size()[0]) + int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        total0 = total.unsqueeze(0).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        L2_distance = ((total0-total1)**2).sum(2)\n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul**i)\n",
    "                          for i in range(kernel_num)]\n",
    "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n",
    "                      for bandwidth_temp in bandwidth_list]\n",
    "        return sum(kernel_val)\n",
    "\n",
    "    def linear_mmd2(self, f_of_X, f_of_Y):\n",
    "        loss = 0.0\n",
    "        delta = f_of_X.float().mean(0) - f_of_Y.float().mean(0)\n",
    "        loss = delta.dot(delta.T)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        if self.kernel_type == 'linear':\n",
    "            return self.linear_mmd2(source, target)\n",
    "        elif self.kernel_type == 'rbf':\n",
    "            batch_size = int(source.size()[0])\n",
    "            kernels = self.guassian_kernel(\n",
    "                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
    "            XX = torch.mean(kernels[:batch_size, :batch_size])\n",
    "            YY = torch.mean(kernels[batch_size:, batch_size:])\n",
    "            XY = torch.mean(kernels[:batch_size, batch_size:])\n",
    "            YX = torch.mean(kernels[batch_size:, :batch_size])\n",
    "            loss = torch.mean(XX + YY - XY - YX)\n",
    "            return loss\n",
    "def loss_function(recon_x, x, mu, logvar,recon_x1, x1, mu1, logvar1):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    MMD = MMDLoss()\n",
    "    l = loss_function1(recon_x, x, mu, logvar)\n",
    "    l1 = loss_function1(recon_x1, x1, mu1, logvar1)    \n",
    "    return l+l1+0.5*MMD(source=mu, target=mu1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a76b28d-df13-4c1b-ac16-1f40f39cc0ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/3618 (0%)] Loss: 8090.896484 time:4.00s\n",
      "Train Epoch: 0 [1280/3618 (34%)] Loss: 7565.028320 time:6.00s\n",
      "Train Epoch: 0 [2560/3618 (69%)] Loss: 6914.221191 time:9.00s\n",
      "====> Epoch: 0 Average loss: 7271.3367\n",
      "Train Epoch: 1 [0/3618 (0%)] Loss: 6222.801270 time:14.00s\n",
      "Train Epoch: 1 [1280/3618 (34%)] Loss: 5627.159180 time:17.00s\n",
      "Train Epoch: 1 [2560/3618 (69%)] Loss: 5167.737305 time:20.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 1 Average loss: 5447.1001\n",
      "Train Epoch: 2 [1280/3618 (34%)] Loss: 4584.595703 time:28.00s\n",
      "Train Epoch: 2 [2560/3618 (69%)] Loss: 4364.247070 time:30.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 2 Average loss: 4489.5600\n",
      "Train Epoch: 3 [0/3618 (0%)] Loss: 4029.337646 time:35.00s\n",
      "Train Epoch: 3 [1280/3618 (34%)] Loss: 3986.665039 time:38.00s\n",
      "Train Epoch: 3 [2560/3618 (69%)] Loss: 3743.084229 time:41.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 3 Average loss: 3892.6618\n",
      "Train Epoch: 4 [0/3618 (0%)] Loss: 3810.242676 time:46.00s\n",
      "Train Epoch: 4 [1280/3618 (34%)] Loss: 3430.180176 time:49.00s\n",
      "Train Epoch: 4 [2560/3618 (69%)] Loss: 3509.752441 time:51.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 4 Average loss: 3514.3652\n",
      "Train Epoch: 5 [0/3618 (0%)] Loss: 3494.954590 time:56.00s\n",
      "Train Epoch: 5 [1280/3618 (34%)] Loss: 3198.999512 time:59.00s\n",
      "Train Epoch: 5 [2560/3618 (69%)] Loss: 3157.541504 time:62.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 5 Average loss: 3258.8613\n",
      "Train Epoch: 6 [0/3618 (0%)] Loss: 3281.072754 time:67.00s\n",
      "Train Epoch: 6 [1280/3618 (34%)] Loss: 3104.513672 time:70.00s\n",
      "Train Epoch: 6 [2560/3618 (69%)] Loss: 2988.387695 time:73.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 6 Average loss: 3074.4041\n",
      "Train Epoch: 7 [0/3618 (0%)] Loss: 3123.760254 time:78.00s\n",
      "Train Epoch: 7 [1280/3618 (34%)] Loss: 3038.909180 time:80.00s\n",
      "Train Epoch: 7 [2560/3618 (69%)] Loss: 2961.790527 time:83.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 7 Average loss: 2945.1553\n",
      "Train Epoch: 8 [0/3618 (0%)] Loss: 2967.859375 time:88.00s\n",
      "Train Epoch: 8 [1280/3618 (34%)] Loss: 2832.087891 time:90.00s\n",
      "Train Epoch: 8 [2560/3618 (69%)] Loss: 2730.528320 time:93.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 8 Average loss: 2847.8010\n",
      "Train Epoch: 9 [0/3618 (0%)] Loss: 2698.864502 time:98.00s\n",
      "Train Epoch: 9 [1280/3618 (34%)] Loss: 2631.128906 time:101.00s\n",
      "Train Epoch: 9 [2560/3618 (69%)] Loss: 2697.479736 time:104.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 9 Average loss: 2769.1546\n",
      "Train Epoch: 10 [0/3618 (0%)] Loss: 2697.428711 time:109.00s\n",
      "Train Epoch: 10 [1280/3618 (34%)] Loss: 2762.758789 time:112.00s\n",
      "Train Epoch: 10 [2560/3618 (69%)] Loss: 2847.232422 time:115.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 10 Average loss: 2702.9906\n",
      "Train Epoch: 11 [0/3618 (0%)] Loss: 2670.660400 time:119.00s\n",
      "Train Epoch: 11 [1280/3618 (34%)] Loss: 2699.587891 time:122.00s\n",
      "Train Epoch: 11 [2560/3618 (69%)] Loss: 2631.923096 time:125.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 11 Average loss: 2644.8528\n",
      "Train Epoch: 12 [0/3618 (0%)] Loss: 2738.083008 time:130.00s\n",
      "Train Epoch: 12 [1280/3618 (34%)] Loss: 2509.039307 time:132.00s\n",
      "Train Epoch: 12 [2560/3618 (69%)] Loss: 2551.958496 time:135.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 12 Average loss: 2595.2067\n",
      "Train Epoch: 13 [0/3618 (0%)] Loss: 2633.624512 time:140.00s\n",
      "Train Epoch: 13 [1280/3618 (34%)] Loss: 2651.314453 time:143.00s\n",
      "Train Epoch: 13 [2560/3618 (69%)] Loss: 2398.130127 time:146.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 13 Average loss: 2551.0676\n",
      "Train Epoch: 14 [0/3618 (0%)] Loss: 2484.647461 time:150.00s\n",
      "Train Epoch: 14 [1280/3618 (34%)] Loss: 2509.266113 time:153.00s\n",
      "Train Epoch: 14 [2560/3618 (69%)] Loss: 2585.115479 time:156.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 14 Average loss: 2512.3166\n",
      "Train Epoch: 15 [0/3618 (0%)] Loss: 2472.553467 time:161.00s\n",
      "Train Epoch: 15 [1280/3618 (34%)] Loss: 2417.000488 time:163.00s\n",
      "Train Epoch: 15 [2560/3618 (69%)] Loss: 2566.108887 time:166.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 15 Average loss: 2477.4834\n",
      "Train Epoch: 16 [0/3618 (0%)] Loss: 2399.713135 time:171.00s\n",
      "Train Epoch: 16 [1280/3618 (34%)] Loss: 2506.012207 time:174.00s\n",
      "Train Epoch: 16 [2560/3618 (69%)] Loss: 2388.815674 time:177.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 16 Average loss: 2445.3356\n",
      "Train Epoch: 17 [0/3618 (0%)] Loss: 2426.645020 time:181.00s\n",
      "Train Epoch: 17 [1280/3618 (34%)] Loss: 2391.099121 time:185.00s\n",
      "Train Epoch: 17 [2560/3618 (69%)] Loss: 2397.645752 time:187.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 17 Average loss: 2418.0844\n",
      "Train Epoch: 18 [0/3618 (0%)] Loss: 2358.848877 time:193.00s\n",
      "Train Epoch: 18 [1280/3618 (34%)] Loss: 2577.958008 time:196.00s\n",
      "Train Epoch: 18 [2560/3618 (69%)] Loss: 2330.612305 time:198.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 18 Average loss: 2392.1633\n",
      "Train Epoch: 19 [0/3618 (0%)] Loss: 2390.419434 time:204.00s\n",
      "Train Epoch: 19 [1280/3618 (34%)] Loss: 2338.681641 time:206.00s\n",
      "Train Epoch: 19 [2560/3618 (69%)] Loss: 2431.210449 time:209.00s\n",
      "Save Best Model!\n",
      "====> Epoch: 19 Average loss: 2367.6142\n"
     ]
    }
   ],
   "source": [
    "train_loss11 = open('./train_proces/train_1.txt', 'w')\n",
    "train_data = MyDataset(datatxt='train_tal1.txt', transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size = batch_size, shuffle=True,num_workers=20)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "strattime = datetime.datetime.now()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        img,oimg,_ = data\n",
    "        img = Variable(img)\n",
    "        img = img.to(device)\n",
    "        oimg = Variable(oimg)\n",
    "        oimg = oimg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        cimg, mu, lov = model(img)\n",
    "        ocimg, omu, olov = model(oimg)\n",
    "        loss = loss_function(cimg, img, mu, lov, ocimg, oimg, omu, olov)\n",
    "        loss.backward()\n",
    "        # train_loss += loss.data[0]\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            endtime = datetime.datetime.now()\n",
    "            asd = str('Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f} time:{:.2f}s'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(img),\n",
    "                len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(img),\n",
    "                (endtime-strattime).seconds))\n",
    "            print(asd)\n",
    "            train_loss11.write(asd+'\\n')\n",
    "            # torch.save(model, './model/b_vae'+str(epoch)+'_'+str(batch_idx)+'.pth')\n",
    "    if epoch == 0:\n",
    "        best_loss = train_loss / len(train_loader.dataset)\n",
    "    if epoch > 0 and best_loss > train_loss / len(train_loader.dataset):\n",
    "        best_loss = train_loss / len(train_loader.dataset)\n",
    "        asds = 'Save Best Model!'\n",
    "        print(asds)\n",
    "        train_loss11.write(asds+'\\n')\n",
    "        torch.save(model, './model/vae_'+str(num_var)+'_'+str(k)+'_best.pth')\n",
    "    asds = str('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset)))\n",
    "    print(asds)\n",
    "    train_loss11.write(asds+'\\n')\n",
    "train_loss11.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d65099c-9ccd-4a52-b46e-0238f5c43cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fspecial_gauss_1d(size, sigma):\n",
    "    coords = torch.arange(size).to(dtype=torch.float)\n",
    "    coords -= size//2\n",
    "    g = torch.exp(-(coords**2) / (2*sigma**2))\n",
    "    g /= g.sum()\n",
    "    return g.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "def gaussian_filter(input, win):\n",
    "    N, C, H, W = input.shape\n",
    "    out = F.conv2d(input, win, stride=1, padding=0, groups=C)\n",
    "    out = F.conv2d(out, win.transpose(2, 3), stride=1, padding=0, groups=C)\n",
    "    return out\n",
    "\n",
    "def _ssim(X, Y, win, data_range=1023, size_average=True, full=False):\n",
    "    K1 = 0.01\n",
    "    K2 = 0.03\n",
    "    batch, channel, height, width = X.shape\n",
    "    compensation = 1.0\n",
    "\n",
    "    C1 = (K1 * data_range)**2\n",
    "    C2 = (K2 * data_range)**2\n",
    "\n",
    "    win = win.to(X.device, dtype=X.dtype)\n",
    "\n",
    "    mu1 = gaussian_filter(X, win)\n",
    "    mu2 = gaussian_filter(Y, win)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = compensation * ( gaussian_filter(X * X, win) - mu1_sq )\n",
    "    sigma2_sq = compensation * ( gaussian_filter(Y * Y, win) - mu2_sq )\n",
    "    sigma12   = compensation * ( gaussian_filter(X * Y, win) - mu1_mu2 )\n",
    "\n",
    "    cs_map = (2 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2)\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) / (mu1_sq + mu2_sq + C1)) * cs_map\n",
    "\n",
    "    if size_average:\n",
    "        ssim_val = ssim_map.mean()\n",
    "        cs = cs_map.mean()\n",
    "    else:\n",
    "        ssim_val = ssim_map.mean(-1).mean(-1).mean(-1)  # reduce along CHW\n",
    "        cs = cs_map.mean(-1).mean(-1).mean(-1)\n",
    "\n",
    "    if full:\n",
    "        return ssim_val, cs\n",
    "    else:\n",
    "        return ssim_val\n",
    "\n",
    "def ssim(X, Y, win_size=11, win_sigma=10, win=None, data_range=1, size_average=True, full=False):\n",
    "\n",
    "    if len(X.shape) != 4:\n",
    "        raise ValueError('Input images must 4-d tensor.')\n",
    "\n",
    "    if not X.type() == Y.type():\n",
    "        raise ValueError('Input images must have the same dtype.')\n",
    "\n",
    "    if not X.shape == Y.shape:\n",
    "        raise ValueError('Input images must have the same dimensions.')\n",
    "\n",
    "    if not (win_size % 2 == 1):\n",
    "        raise ValueError('Window size must be odd.')\n",
    "\n",
    "    win_sigma = win_sigma\n",
    "    if win is None:\n",
    "        win = _fspecial_gauss_1d(win_size, win_sigma)\n",
    "        win = win.repeat(X.shape[1], 1, 1, 1)\n",
    "    else:\n",
    "        win_size = win.shape[-1]\n",
    "\n",
    "    ssim_val, cs = _ssim(X, Y,\n",
    "                         win=win,\n",
    "                         data_range=data_range,\n",
    "                         size_average=False,\n",
    "                         full=True)\n",
    "    if size_average:\n",
    "        ssim_val = ssim_val.mean()\n",
    "        cs = cs.mean()\n",
    "\n",
    "    if full:\n",
    "        return ssim_val, cs\n",
    "    else:\n",
    "        return ssim_val\n",
    "\n",
    "\n",
    "    def __init__(self, win_size=11, win_sigma=1.5, data_range=255, size_average=True, channel=3, weights=None):\n",
    "        super(MS_SSIM, self).__init__()\n",
    "        self.win = _fspecial_gauss_1d(\n",
    "            win_size, win_sigma).repeat(channel, 1, 1, 1)\n",
    "        self.size_average = size_average\n",
    "        self.data_range = data_range\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return ms_ssim(X, Y, win=self.win, size_average=self.size_average, data_range=self.data_range, weights=self.weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65e494c9-5898-4992-a3ff-b686d734aa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 29/29 [00:13<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3618\n"
     ]
    }
   ],
   "source": [
    "train_data = MyDataset(datatxt='train_tal1.txt', transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size = batch_size, shuffle=False,num_workers=20)\n",
    "\n",
    "if not os.path.exists('./result'):\n",
    "    os.mkdir('./result')\n",
    "model.eval()\n",
    "from tqdm import tqdm\n",
    "sssi = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(tqdm(train_loader)):\n",
    "        img,oimg,fn = data\n",
    "        img = Variable(img)\n",
    "        img = img.to(device)\n",
    "        oimg = Variable(oimg)\n",
    "        oimg = oimg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cimg, mu, lov = model(img)\n",
    "        ocimg, omu, olov = model(oimg)\n",
    "\n",
    "        for i in range(len(img)):\n",
    "            ssim_val = ssim(img[i].unsqueeze(0), cimg[i].unsqueeze(0), data_range=1, size_average=True,)\n",
    "            qw = [fn[0][i]]\n",
    "            qw.append(ssim_val.cpu().detach().numpy())\n",
    "            qw.extend(mu[i].cpu().detach().numpy())\n",
    "            qw.append(fn[1][i])\n",
    "            ssim_val = ssim(oimg[i].unsqueeze(0), ocimg[i].unsqueeze(0), data_range=1, size_average=True,)\n",
    "            qw.append(ssim_val.cpu().detach().numpy())\n",
    "            qw.extend(omu[i].cpu().detach().numpy())\n",
    "            sssi.append(qw)\n",
    "\n",
    "    dd = np.array(sssi)\n",
    "    print(len(dd))\n",
    "    # np.save(pt+'result_ssim.npy',dd)\n",
    "    np.save('./result/resu_'+str(num_var)+'_'+str(k)+'_all_10_15.npy',dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ac87a1-83ff-4fe4-8401-62b8e6cc8ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
